{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "MVARL19_part1 (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3WM-hVOPfo",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9_DLZvWQzhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnzUJeyJOPfq",
        "colab_type": "text"
      },
      "source": [
        "## MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWJSw_uOPfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import numpy as np\n",
        "from scipy.special import softmax # for SARSA\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from cliffwalk import CliffWalk\n",
        "from test_env import ToyEnv1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym-B_4HaOPfu",
        "colab_type": "text"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVR5qYoLOPfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = CliffWalk(proba_succ=0.98)\n",
        "\n",
        "####################################################################################\n",
        "# You probably want to test smaller enviroments before\n",
        "#env = ToyEnv1(gamma=0.95)\n",
        "####################################################################################\n",
        "\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n",
        "print(env.R.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUlNvT3cOPfx",
        "colab_type": "text"
      },
      "source": [
        "## Question 1: Value iteration\n",
        "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
        "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition (reuse function implemented in 1)\n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|Q_n - Q^\\star\\|_{\\infty} = \\max_{s,a} |Q_n(s,a) - Q^\\star(s,a)|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc3KMEPOTQKF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8TLRx6MOPfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "def bellman_operator(Q0, Ns, Na, R, P, gamma):\n",
        "    \"Ns: Number of states: \"\n",
        "    \"Na: Number of actions: \"\n",
        "    \"P is P[s,a,s'] \"\n",
        "    \"R: reward R[s,a,s']\"\n",
        "    greedy_policy = np.argmax(Q0 , axis=1)\n",
        "    Q1 = np.sum(R*P,axis = 2) + gamma* np.sum(P * Q0[np.arange(len(Q0)),greedy_policy] , axis = 2)\n",
        "    greedy_policy = np.argmax(Q1 , axis=1)\n",
        "    return Q1, greedy_policy\n",
        "\n",
        "#    r = np.zeros(shape = (Ns,Na))\n",
        "#    greedy_policy = []\n",
        "#    g = 0\n",
        "#    for s in range(Ns):\n",
        "#      for a in range(Na):\n",
        "#        for sp in range(Ns):\n",
        "#          r[s,a] += R[s,a,sp]*P[s,a,sp] \n",
        "#    Q1 = np.zeros(shape = (Ns,Na))\n",
        "#    for s in range(Ns):      \n",
        "#      for a in range(Na):\n",
        "#        val=0\n",
        "#        for sp in range(Ns):\n",
        "#          val += gamma*P[s,a,sp]*Q0[s,a]\n",
        "#        val += r[s,a]\n",
        "#        if val > Q1[s,a]:\n",
        "#          Q1[s,a] = val\n",
        "#          greedy= a\n",
        "#        greedy_policy.append(greedy)\n",
        "#      g += 1\n",
        "#    return Q1, greedy_policy\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj65cQk5OPf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 2\n",
        "# --------------\n",
        "\n",
        "def value_iteration(Q0, env, epsilon=1e-5):\n",
        "    \n",
        "    gamma = env.gamma\n",
        "    k = 1\n",
        "    Q_history = [Q0]\n",
        "    Q_history.append( bellman_operator(Q0, env.Ns, env.Na, env.R, env.P, gamma)[0])\n",
        "    while np.max(np.linalg.norm(Q_history[k]-Q_history[k-1], ord = np.inf)) > epsilon:    \n",
        "      Q, greedy_policy = bellman_operator(Q_history[-1], env.Ns, env.Na, env.R, env.P, gamma)\n",
        "      Q_history.append(Q)\n",
        "      k+=1  \n",
        "    return Q, greedy_policy, Q_history\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_lBe6q6OPf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 3\n",
        "# --------------\n",
        "with open(\"./mvarl_hands_on/data/Q_opts.json\", \"r\") as fp:\n",
        "    Qopts = json.load(fp)\n",
        "Qstar = Qopts[\"{}_{}\".format(type(env).__name__,env.gamma)]\n",
        "\n",
        "\n",
        "Q0 = np.zeros((env.Ns,env.Na))\n",
        "Q, greedy_policy, Q_history = value_iteration(Q0, env, epsilon=1e-5)\n",
        "iteration = np.arange(len(Q_history)-1)\n",
        "norm_values = []\n",
        "for k in range(1, len(Q_history)):\n",
        "  norm_values.append(np.linalg.norm(Q_history[k]-Qstar, ord = np.inf))\n",
        "\n",
        "plt.plot(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbzFs3tDvcJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(50):\n",
        "    action = greedy_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azfXmRzZOPf4",
        "colab_type": "text"
      },
      "source": [
        "## Question 2: Q learning\n",
        "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
        "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the sum of rewards as a function of iteration\n",
        "\n",
        "\n",
        "$\\epsilon$-greedy policy:\n",
        "$$\n",
        "\\pi(s) = \\begin{cases}\n",
        "\\max_a Q(s,a) & \\text{w.p.} \\epsilon\\\\\n",
        "\\text{random action} & \\text{w.p.} 1- \\epsilon\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t3WqIt-OPf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Learning\n",
        "# ---------------------------\n",
        "\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self,env, epsilon):\n",
        "        self.Ns, self.Na = env.Ns, env.Na\n",
        "        self.gamma = env.gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = np.zeros((self.Ns, self.Na))\n",
        "        self.N = np.zeros((self.Ns, self.Na))\n",
        "    \n",
        "    def sample_action(self, state, greedy):\n",
        "        p = np.random.rand(1)\n",
        "        if p > self.epsilon:\n",
        "          return greedy[state]\n",
        "        else:\n",
        "          return np.random.randint(self.Na)\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "        self.N[state,action] += 1\n",
        "        delta = reward + self.gamma * np.max(self.Q[next_state,:]) - self.Q[state, action]\n",
        "        l_rate = 1/self.N[state,action]\n",
        "        self.Q[state,action] = self.Q[state,action] + l_rate* delta\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW9YFGlhWClW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.append(np.zeros(256),4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKTc5nWIOPf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "env = CliffWalk(proba_succ=0.98)\n",
        "# Number of Q learning steps\n",
        "max_steps = int(1e5)  \n",
        "# max_steps = 10\n",
        "\n",
        "Q0 = np.zeros((env.Ns, env.Na))\n",
        "# Use the previous code to verify the correctness of q learning\n",
        "Q_opt, pi_opt, _ = value_iteration(Q0, env, epsilon=1e-8)\n",
        "\n",
        "epsilon = 0.01\n",
        "ql = QLearning(env, epsilon)\n",
        "\n",
        "# main algorithmic loop\n",
        "norm_values = []\n",
        "rewards = [0]\n",
        "t = 0\n",
        "while t < max_steps:\n",
        "    state = env.state\n",
        "    greedy_policy = np.argmax(ql.Q, axis=1)\n",
        "    action = ql.sample_action(state, greedy_policy)\n",
        "    next_step, reward, done, info = env.step(action)\n",
        "    norm_values.append(np.abs(ql.Q - Q_opt).mean())\n",
        "    rewards.append(rewards[-1] + reward)\n",
        "    \n",
        "    ql.update(state, action, next_state, reward)\n",
        "    t = t + 1\n",
        "\n",
        "\n",
        "    \n",
        "print(env.render())\n",
        "print(\"optimal policy: \", pi_opt)\n",
        "greedy_policy = np.argmax(ql.Q, axis=1)\n",
        "print(\"learnt policy:\", greedy_policy)\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title(\"Sum of Rewards\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-5JT-2LOMx",
        "colab_type": "text"
      },
      "source": [
        "# how confident are you in the performance of the algorithm? maybe a single run is not enough\n",
        "\n",
        "\n",
        "Not very confident because the error is still high after 'max_step' iterations.\n",
        "This is due to the fact that there are some states that were not explored that \n",
        "much and then were not updated to know what will be the optimal policy. \n",
        "The Robbins-Monro conditions impose that all states have to be visited\n",
        " infinitely often in order to converge towards the optimal Q and therefore\n",
        "  the optimal value function.\n",
        "If the MDP doesn't explore well all the states, the algorithm will converge\n",
        " to the optimal policy only if the initial state distribution is large \n",
        " in order to start from all the states.\n",
        "\n",
        " In addition, the optimal policy is to go along the cliff. But the \n",
        " epsilon-greedy algorithm will allow us to explore differents states and then \n",
        " to fall from the cliff. So the best policy could not be optimal and we will \n",
        " prefer to visit farthest states from the cliff than taking risks and fall. \n",
        "\n"
      ]
    }
  ]
}